# ğŸ“„ Docâ€‘Qâ€‘Aâ€‘RAG

**Doc-Q-A-RAG** is a modular **Retrievalâ€‘Augmented Generation (RAG)** pipeline for intelligent question-answering over documents. It leverages vector embeddings, semantic search, and powerful LLMs like GPTâ€‘4 to produce context-aware, accurate responses directly sourced from the document contents.

> âš ï¸ **Note:** This project is currently **in active development**. Core components are being built and tested, and contributions or suggestions are welcome!

---

## ğŸš€ Features

- **ğŸ“š Multi-format Document Input** â€” Supports ingestion of PDF, TXT, DOCX files (planned).
- **ğŸ” Chunking + Embeddings** â€” Splits documents into semantically useful chunks and embeds them using OpenAI embeddings (or HuggingFace).
- **âš¡ Vector Search (FAISS)** â€” Fast and scalable similarity search using dense vector indexes.
- **ğŸ§  LLM-Driven Answers** â€” Uses GPT models to synthesize answers from retrieved content.
- **ğŸ’¾ (Optional) Caching** â€” Support for caching previously answered queries to reduce latency.
- **ğŸ§ª Modular & Extensible** â€” Easy to swap in custom retrievers, chunkers, or LLMs.

---

## ğŸ“¦ Project Status

| Component            | Status         |
|----------------------|----------------|
| Document Ingestion   | âœ… Basic support for TXT/PDF |
| Embedding Generator  | âœ… Working with OpenAI Embeddings |
| Vector Store (FAISS) | âœ… Integrated |
| Question Answering   | âœ… Basic GPT-3.5/4 Support |
| Web/CLI Interface    | ğŸš§ In Progress |
| Chunking Improvements| ğŸš§ Being optimized |
| Evaluation Pipeline  | â³ Planned |
| LangChain Support    | â³ Exploring |
| LLM Models           | â³ Exploring |
| Dataset Expansion    | â³ Exploring |
---

## ğŸ› ï¸ Installation

### ğŸ”§ Prerequisites

- Python 3.8+
- `faiss-cpu` or `faiss-gpu`
- OpenAI API key

### ğŸ§© Setup

```bash
git clone https://github.com/VIROOPAKSHC/Doc-Q-A-RAG.git
cd Doc-Q-A-RAG
python3 -m venv venv
source venv/bin/activate      # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

Create a .env file:
```ini
OPENAI_API_KEY=your_openai_key_here
```

### âš™ï¸ Usage

#### ğŸ“ Document Indexing
```bash
python app.py --docs path/to/docs/*.pdf
```

#### â“ Ask Questions
```bash
python qa.py --query "What are the steps in the RAG pipeline?"
```
Youâ€™ll get an answer generated from the most relevant parts of the ingested documents.

### ğŸ“ Project Structure

```text
Doc-Q-A-RAG/
â”œâ”€â”€ app.py              # Ingest and embed documents
â”œâ”€â”€ qa.py               # Query interface
â”œâ”€â”€ retriever.py        # FAISS-based vector search
â”œâ”€â”€ embeddings.py       # OpenAI/HF embedding utilities
â”œâ”€â”€ chunker.py          # Document chunking logic
â”œâ”€â”€ cache.py            # Optional caching module
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # You're reading it :)
```

### ğŸ§  Acknowledgments
- [RAG: Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401)
- [FAISS by Facebook AI](https://github.com/facebookresearch/faiss)
- [LangChain](https://github.com/hwchase17/langchain)
- [LlamaIndex](https://github.com/jerryjliu/llama_index)

### ğŸ‘¤ Author
Viroopaksh C
AI/ML Researcher & Engineer
[ğŸ”— LinkedIn](https://www.linkedin.com/in/viroopaksh-chekuri) | âœ‰ï¸ Open to collaboration!
